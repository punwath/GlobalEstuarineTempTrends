{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cea046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import geopandas\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from numpy import absolute\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pymannkendall as mk\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0347249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCSV(path):\n",
    "    all_filenames = glob.glob(path + \"/*.csv\")\n",
    "    merge = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "    return merge\n",
    "\n",
    "def pull_clean (data):\n",
    "    data =data.rename(columns={\"Blue\": \"blue\", \"system:index\": \"LS_ID\", \"TIR1\":\"LS_Temp\" })\n",
    "    \n",
    "    data = data.dropna(subset=['blue'])\n",
    "    \n",
    "    data[['sat', 'pathrow', 'date', 'junk']]  = data['LS_ID'].str.split('L', n = 1, expand = True)[1].str.split('_', n =4 , expand = True)\n",
    "    data = data.drop(columns=['junk'])\n",
    "    data['sat'] = 'L'+data['sat'] \n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"]).dt.date\n",
    "    data[\"month\"] = pd.to_datetime(data[\"date\"]).dt.month\n",
    "    data[\"year\"] = pd.to_datetime(data[\"date\"]).dt.year\n",
    "\n",
    "    data['ST_QA'] =  data['ST_QA'] * 0.01\n",
    "    data['ST_CDIST'] =  data['ST_CDIST'] * 0.01\n",
    "    data = data[(data.ST_CDIST > 0.1) & (data.ST_QA < 273.15)]\n",
    "    data['LS_Temp'] = data['LS_Temp']-273.15\n",
    "    data = data[(data.LS_Temp > 0) & (data.LS_Temp < 35)]\n",
    "    data['key'] = data['p_ID'].astype(str)+data['month'].astype(str)\n",
    "    \n",
    "\n",
    "    threshold = data.groupby(['p_ID','month'])['LS_Temp'].describe()[['mean','max', 'min', '25%', '75%']].reset_index()\n",
    "    \n",
    "    threshold['key'] = threshold ['p_ID'].astype(str)+threshold ['month'].astype(str)\n",
    "    threshold ['iqr'] = threshold ['75%'] - threshold ['25%']\n",
    "    threshold ['iqr_upper_bound'] = threshold['75%'] + 1.5*threshold ['iqr']\n",
    "    threshold ['iqr_lower_bound'] = threshold['25%'] - 1.5*threshold ['iqr']\n",
    "    threshold = threshold[['key', 'iqr_upper_bound', 'iqr_lower_bound']]\n",
    "    \n",
    "    #threshold['key'] = threshold['key'].astype(str)\n",
    "    #data['key'] = data['key'].astype(str)\n",
    "    data = data.merge(threshold, on='key', how='left')\n",
    "    data = data[(data.LS_Temp < data.iqr_upper_bound) & (data.LS_Temp > data.iqr_lower_bound)]\n",
    " \n",
    "\n",
    "    #data = data[['key','LS_ID','pathrow','sat','blue', 'green', 'nir', 'red', 'swir1', 'swir2', 'LS_Temp','CLOUD_COVER','ST_CDIST', 'ST_DRAD', 'ST_EMIS', 'ST_EMSD', 'ST_QA', 'ST_TRAD', 'ST_URAD', \n",
    "    #    'SUN_AZIMUTH', 'SUN_ELEVATION', 'date', 'year', 'month', 'ID_NUMBER', 'p_ID']]\n",
    "    data=data[~(data.sat=='LC09')]\n",
    "    #data = data[['LS_ID','pathrow','sat','blue', 'LS_Temp','ST_CDIST', 'ST_QA', 'date', 'year', 'month', 'ID_NUMBER', 'p_ID']]\n",
    "\n",
    "    return data\n",
    "\n",
    "def mktest(x):\n",
    "    result = mk.original_test(x)\n",
    "    return pd.Series(result, index=result._fields)\n",
    "\n",
    "def seasonal_mktest(x):\n",
    "    result = mk.seasonal_test(x)\n",
    "    return pd.Series(result, index=result._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190d240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\Estuary Temperature\\global sr\\Estuary_collection2\"\n",
    "sr = mergeCSV(path)\n",
    "#sr = sr.drop_duplicates()\n",
    "sr.to_csv(\"Global_estuary_sr_clean_final_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de86c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.read_csv(\"Global_estuary_sr_clean_final_raw.csv\")\n",
    "sr = pull_clean (sr)\n",
    "sr.to_csv(\"Global_estuary_sr_clean_final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = sr[(sr.ST_CDIST > 0.2)]#0.2 is good\n",
    "sr['Corrected_LS_Temp'] = 0.340+(0.967*sr['LS_Temp'])\n",
    "sr = sr.drop_duplicates()\n",
    "sr.to_csv(\"Global_estuary_sr_clean_final_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655768fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.read_csv(\"Global_estuary_sr_clean_final_v2.csv\")\n",
    "sr['Blue']=sr['blue']\n",
    "sr[['LS_ID', 'Aerosol', 'Blue', 'Green', 'Red', 'Nir','NirSD','Swir1', 'Swir2','CLOUD_COVER','ST_CDIST',\n",
    "    'ST_DRAD', 'ST_EMIS', 'ST_EMSD','SUN_AZIMUTH','SUN_ELEVATION',\n",
    "    'LS_Temp','Corrected_LS_Temp', 'sat', 'pathrow', 'date', 'month', 'year',\n",
    "    'CONTINENT', 'COUNTRY', 'p_ID','ID_NUMBER']].to_csv(\"Global_estuary_sr_clean_final_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3acb901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Estuary 1060\n",
      "Number of sites 195566\n",
      "year min 1985\n",
      "year min 2022\n",
      "(64375639, 27)\n"
     ]
    }
   ],
   "source": [
    "sr = pd.read_csv(r\"D:\\Estuary Temperature\\global sr\\final\\Global_estuary_sr_clean_final_v3.csv\")\n",
    "# Count the number of estuary detected by landsat\n",
    "print(\"Number of Estuary\",len(sr['ID_NUMBER'].unique()))\n",
    "print(\"Number of sites\",len(sr['p_ID'].unique()))\n",
    "print(\"year min\", sr['year'].min())\n",
    "print(\"year min\", sr['year'].max())\n",
    "print(sr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4ae9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count how many points per seasonal temperature per estuary\n",
    "estuary_shp = geopandas.read_file(r\"D:\\Estuary Temperature\\global sr\\final\\shp\\estuary_centroid.shp\")\n",
    "def find_season(month, hemisphere):\n",
    "    if hemisphere == 'Southern':\n",
    "        season_month_south = {\n",
    "            12:'Summer', 1:'Summer', 2:'Summer',\n",
    "            3:'Autumn', 4:'Autumn', 5:'Autumn',\n",
    "            6:'Winter', 7:'Winter', 8:'Winter',\n",
    "            9:'Spring', 10:'Spring', 11:'Spring'}\n",
    "        return season_month_south.get(month)\n",
    "        \n",
    "    elif hemisphere == 'Northern':\n",
    "        season_month_north = {\n",
    "            12:'Winter', 1:'Winter', 2:'Winter',\n",
    "            3:'Spring', 4:'Spring', 5:'Spring',\n",
    "            6:'Summer', 7:'Summer', 8:'Summer',\n",
    "            9:'Autumn', 10:'Autumn', 11:'Autumn'}\n",
    "        return season_month_north.get(month)\n",
    "    else:\n",
    "        print('Invalid selection. Please select a hemisphere and try again')\n",
    "\n",
    "\n",
    "sr_year_estuary = sr[['ID_NUMBER', 'p_ID','year','month', 'Corrected_LS_Temp']]\n",
    "sr_year_estuary = sr_year_estuary.merge(estuary_shp, on='ID_NUMBER', how='left')\n",
    "south = sr_year_estuary[sr_year_estuary['INPUT_LAT'] <0]\n",
    "north = sr_year_estuary[sr_year_estuary['INPUT_LAT'] >0]\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Southern'\n",
    "for month in south['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "south['Season'] = season_list\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Southern'\n",
    "for month in north['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "north['Season'] = season_list\n",
    "\n",
    "sr_year_estuary = pd.concat([north, south])\n",
    "\n",
    "a = sr_year_estuary.groupby(['ID_NUMBER', 'year','Season'])['Corrected_LS_Temp'].size().reset_index()\n",
    "a['key'] = a['ID_NUMBER'].astype(str) +\"_\"+a['Season']\n",
    "a = a.merge(estuary_shp, on='ID_NUMBER', how='left')\n",
    "b = a.groupby(['ID_NUMBER','Season'])['Corrected_LS_Temp'].max().reset_index()\n",
    "b['key'] = b['ID_NUMBER'].astype(str) +\"_\"+b['Season']\n",
    "\n",
    "c = a.merge(b, on='key', how='left')\n",
    "c['percentage'] = c['Corrected_LS_Temp_x']/c['Corrected_LS_Temp_y']\n",
    "c = c[c['percentage'] > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc63a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "estuary_shp = geopandas.read_file(r\"D:\\Estuary Temperature\\global sr\\final\\shp\\estuary_centroid.shp\")\n",
    "def find_season(month, hemisphere):\n",
    "    if hemisphere == 'Southern':\n",
    "        season_month_south = {\n",
    "            12:'Summer', 1:'Summer', 2:'Summer',\n",
    "            3:'Autumn', 4:'Autumn', 5:'Autumn',\n",
    "            6:'Winter', 7:'Winter', 8:'Winter',\n",
    "            9:'Spring', 10:'Spring', 11:'Spring'}\n",
    "        return season_month_south.get(month)\n",
    "        \n",
    "    elif hemisphere == 'Northern':\n",
    "        season_month_north = {\n",
    "            12:'Winter', 1:'Winter', 2:'Winter',\n",
    "            3:'Spring', 4:'Spring', 5:'Spring',\n",
    "            6:'Summer', 7:'Summer', 8:'Summer',\n",
    "            9:'Autumn', 10:'Autumn', 11:'Autumn'}\n",
    "        return season_month_north.get(month)\n",
    "    else:\n",
    "        print('Invalid selection. Please select a hemisphere and try again')\n",
    "\n",
    "\n",
    "sr_year_estuary = sr[['ID_NUMBER', 'p_ID','year','month', 'Corrected_LS_Temp']]\n",
    "sr_year_estuary = sr_year_estuary.merge(estuary_shp, on='ID_NUMBER', how='left')\n",
    "south = sr_year_estuary[sr_year_estuary['INPUT_LAT'] <0]\n",
    "north = sr_year_estuary[sr_year_estuary['INPUT_LAT'] >0]\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Southern'\n",
    "for month in south['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "south['Season'] = season_list\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Southern'\n",
    "for month in north['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "north['Season'] = season_list\n",
    "\n",
    "sr_year_estuary = pd.concat([north, south])\n",
    "#### More than 70% of water from above calculation\n",
    "sr_year_estuary['key'] = sr_year_estuary['ID_NUMBER'].astype(str) +\"_\"+sr_year_estuary['Season']\n",
    "sr_year_estuary = sr_year_estuary[sr_year_estuary['key'].isin(c.key)]\n",
    "\n",
    "##### More than 3 seasons\n",
    "\n",
    "season = sr_year_estuary[['ID_NUMBER', 'p_ID','year','month','Season', 'Corrected_LS_Temp']].groupby(['ID_NUMBER',  'year','Season'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "season =season.groupby(['ID_NUMBER','year']).size().reset_index(name='counts')\n",
    "season = season[season.counts >3]\n",
    "season['key'] = season['ID_NUMBER'].astype(str) +\"_\"+ season['year'].astype(str) \n",
    "\n",
    "year = sr_year_estuary.groupby(['ID_NUMBER', 'p_ID', 'year','Season'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "year = year.groupby(['ID_NUMBER', 'year','Season'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "\n",
    "year = year.groupby(['ID_NUMBER', 'year'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "year['key'] = year['ID_NUMBER'].astype(str) +\"_\"+ year['year'].astype(str) \n",
    "year=year[year['key'].isin(season['key'])]\n",
    "\n",
    "#More than 10 years of observation\n",
    "count_years = year.groupby(['ID_NUMBER']).size().reset_index(name='counts')\n",
    "count_years = count_years[count_years.counts > 10]\n",
    "year = year[year['ID_NUMBER'].isin(count_years['ID_NUMBER'])]\n",
    "\n",
    "\n",
    "test_each_estuary = year.groupby('ID_NUMBER', as_index=False)['Corrected_LS_Temp'].apply(mktest)\n",
    "\n",
    "test_each_estuary = test_each_estuary.merge(estuary_shp, on='ID_NUMBER', how='left')\n",
    "\n",
    "bins = [-60,-30, 0, 30, 60, 90]\n",
    "num = ['60-30 S ','30-0 S','0-30 N','30-60 N ','60-90 N']\n",
    "test_each_estuary['Latitude Zone']=pd.cut(test_each_estuary['INPUT_LAT'],bins,labels=num)\n",
    "seasons4 = test_each_estuary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca6382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#More than 2 seasons\n",
    "season = sr_year_estuary[['ID_NUMBER', 'p_ID','year','month','Season', 'Corrected_LS_Temp']].groupby(['ID_NUMBER',  'year','Season'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "season =season.groupby(['ID_NUMBER','year']).size().reset_index(name='counts')\n",
    "season = season[season.counts >2]\n",
    "season['key'] = season['ID_NUMBER'].astype(str) +\"_\"+ season['year'].astype(str) \n",
    "\n",
    "year = sr_year_estuary.groupby(['ID_NUMBER', 'p_ID', 'year','Season'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "year = year.groupby(['ID_NUMBER', 'year','Season'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "\n",
    "year = year.groupby(['ID_NUMBER', 'year'])['Corrected_LS_Temp'].mean().reset_index()\n",
    "year['key'] = year['ID_NUMBER'].astype(str) +\"_\"+ year['year'].astype(str) \n",
    "year=year[year['key'].isin(season['key'])]\n",
    "\n",
    "#More than 10 years of observation\n",
    "count_years = year.groupby(['ID_NUMBER']).size().reset_index(name='counts')\n",
    "count_years = count_years[count_years.counts > 10]\n",
    "year = year[year['ID_NUMBER'].isin(count_years['ID_NUMBER'])]\n",
    "\n",
    "\n",
    "test_each_estuary = year.groupby('ID_NUMBER', as_index=False)['Corrected_LS_Temp'].apply(mktest)\n",
    "\n",
    "test_each_estuary = test_each_estuary.merge(estuary_shp, on='ID_NUMBER', how='left')\n",
    "\n",
    "bins = [-60,-30, 0, 30, 60, 90]\n",
    "num = ['60-30 S ','30-0 S','0-30 N','30-60 N ','60-90 N']\n",
    "test_each_estuary['Latitude Zone']=pd.cut(test_each_estuary['INPUT_LAT'],bins,labels=num)\n",
    "season3 = test_each_estuary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545e7b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(677, 29)\n",
      "(958, 29)\n",
      "(677, 29)\n",
      "60.48271\n",
      "72.81475\n"
     ]
    }
   ],
   "source": [
    "print(seasons4.shape)\n",
    "print(season3.shape)\n",
    "print(season3[season3.ID_NUMBER.isin(seasons4['ID_NUMBER'])].shape)\n",
    "print(seasons4.INPUT_LAT.max())\n",
    "print(season3.INPUT_LAT.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ad8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737, 30)\n",
      "69.24593\n"
     ]
    }
   ],
   "source": [
    "#Combine the seasons\n",
    "test_each_estuary = pd.concat([seasons4, season3[season3.INPUT_LAT > 60.48271]]).reset_index()\n",
    "test_each_estuary.to_csv(\"test_each_estuary_final_V2.csv\", index=False)\n",
    "print(test_each_estuary.shape)\n",
    "test_each_estuary = test_each_estuary[~(test_each_estuary.trend=='no trend')]\n",
    "print(test_each_estuary.INPUT_LAT.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb639d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import pandas as pd\n",
    "test_each_estuary = pd.read_csv(r\"test_each_estuary_final_V2.csv\")\n",
    "test_each_estuary = test_each_estuary[test_each_estuary.trend != \"no trend\"]\n",
    "estuary_shp = geopandas.read_file(r\"D:\\Estuary Temperature\\global sr\\final\\shp\\estuary_centroid.shp\")\n",
    "\n",
    "def find_season(month, hemisphere):\n",
    "    if hemisphere == 'Southern':\n",
    "        season_month_south = {\n",
    "            12:'Summer', 1:'Summer', 2:'Summer',\n",
    "            3:'Autumn', 4:'Autumn', 5:'Autumn',\n",
    "            6:'Winter', 7:'Winter', 8:'Winter',\n",
    "            9:'Spring', 10:'Spring', 11:'Spring'}\n",
    "        return season_month_south.get(month)\n",
    "        \n",
    "    elif hemisphere == 'Northern':\n",
    "        season_month_north = {\n",
    "            12:'Winter', 1:'Winter', 2:'Winter',\n",
    "            3:'Spring', 4:'Spring', 5:'Spring',\n",
    "            6:'Summer', 7:'Summer', 8:'Summer',\n",
    "            9:'Autumn', 10:'Autumn', 11:'Autumn'}\n",
    "        return season_month_north.get(month)\n",
    "    else:\n",
    "        print('Invalid selection. Please select a hemisphere and try again')\n",
    "\n",
    "air_temp = pd.read_csv(r\"D:\\GDrive\\er5_weight_scale100.csv\")\n",
    "air_temp= air_temp.dropna(subset=['mean'])\n",
    "air_temp['mean_2m_air_temperature'] = air_temp['mean']\n",
    "\n",
    "air_temp ['year'] = air_temp ['system:index'].str.slice(start=0, stop=4).astype(int)\n",
    "air_temp['month'] = air_temp['system:index'].str.slice(start=4, stop=6).astype(int)\n",
    "air_temp = air_temp[['year', 'month', 'mean_2m_air_temperature','ID_NUMBER']].dropna()\n",
    "air_temp = air_temp[(air_temp.year < 2021) & (air_temp.year > 1984)]\n",
    "air_temp ['mean_2m_air_temperature'] = air_temp ['mean_2m_air_temperature'] - 273.15\n",
    "air_temp\n",
    "\n",
    "air_temp= air_temp.merge(estuary_shp, on='ID_NUMBER', how='left')\n",
    "south = air_temp[air_temp['INPUT_LAT'] <0]\n",
    "north = air_temp[air_temp['INPUT_LAT'] >0]\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Southern'\n",
    "for month in south['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "south['Season'] = season_list\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Southern'\n",
    "for month in north['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "north['Season'] = season_list\n",
    "\n",
    "air_temp = pd.concat([north, south])\n",
    "\n",
    "air_temp = air_temp.groupby([ 'ID_NUMBER','year', 'Season'])['mean_2m_air_temperature'].mean().reset_index()\n",
    "air_temp = air_temp[['ID_NUMBER', 'year', 'mean_2m_air_temperature']].groupby([ 'ID_NUMBER','year'])['mean_2m_air_temperature'].mean().reset_index()\n",
    "\n",
    "\n",
    "air_temp_test  = air_temp.groupby('ID_NUMBER', as_index=False)['mean_2m_air_temperature'].apply(mktest)\n",
    "air_temp_test = air_temp_test[air_temp_test.ID_NUMBER.isin(test_each_estuary.ID_NUMBER)]\n",
    "air_temp_test.groupby('trend').size()\n",
    "air_temp_test.to_csv(\"air_temp_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7bc6725fc7a5374b9a2789f8b4dcc977694f931176284c70495569cae156e8d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
